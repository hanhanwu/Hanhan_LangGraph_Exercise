{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeamT4np6tJHqk6M+y1nzN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhanwu/Hanhan_LangGraph_Exercise/blob/main/AI_for_BI/run_deepseek_r1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "\n",
        "* DeepSeek R1 Model List: https://github.com/deepseek-ai/DeepSeek-R1"
      ],
      "metadata": {
        "id": "5qokAIfWqqui"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jFrbF89oj63B"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install fastapi nest-asyncio pyngrok uvicorn\n",
        "%pip install vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start vLLM Server\n",
        "\n",
        "* On Google Colab, have to use <b>T4 GPU as runtime</b>! üíñ\n",
        "* Reference: https://github.com/naufalhakim23/deepseek-r1-playground/blob/main/deepseek_r1_distill_qwen_fast_api.ipynb"
      ],
      "metadata": {
        "id": "YDOsFUxNska6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model:\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
        "vllm_process = subprocess.Popen([\n",
        "    'vllm',\n",
        "    'serve',  # Subcommand must follow vllm\n",
        "    model,\n",
        "    '--trust-remote-code',\n",
        "    '--dtype', 'half',\n",
        "    '--max-model-len', '16384',\n",
        "    '--enable-chunked-prefill', 'false',\n",
        "    '--tensor-parallel-size', '1'\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)"
      ],
      "metadata": {
        "id": "6HlLRz-Zkd6g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from typing import Tuple\n",
        "import sys\n",
        "\n",
        "def check_vllm_status(url: str = \"http://localhost:8000/health\") -> bool:\n",
        "    \"\"\"Check if VLLM server is running and healthy.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        return response.status_code == 200\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return False\n",
        "\n",
        "def monitor_vllm_process(vllm_process: subprocess.Popen, check_interval: int = 5) -> Tuple[bool, str, str]:\n",
        "    \"\"\"\n",
        "    Monitor VLLM process and return status, stdout, and stderr.\n",
        "    Returns: (success, stdout, stderr)\n",
        "    \"\"\"\n",
        "    print(\"Starting VLLM server monitoring...\")\n",
        "\n",
        "    while vllm_process.poll() is None:  # While process is still running\n",
        "        if check_vllm_status():\n",
        "            print(\"‚úì VLLM server is up and running!\")\n",
        "            return True, \"\", \"\"\n",
        "\n",
        "        print(\"Waiting for VLLM server to start...\")\n",
        "        time.sleep(check_interval)\n",
        "\n",
        "        # Check if there's any output to display\n",
        "        if vllm_process.stdout.readable():\n",
        "            stdout = vllm_process.stdout.read1().decode('utf-8')\n",
        "            if stdout:\n",
        "                print(\"STDOUT:\", stdout)\n",
        "\n",
        "        if vllm_process.stderr.readable():\n",
        "            stderr = vllm_process.stderr.read1().decode('utf-8')\n",
        "            if stderr:\n",
        "                print(\"STDERR:\", stderr)\n",
        "\n",
        "    # If we get here, the process has ended\n",
        "    stdout, stderr = vllm_process.communicate()\n",
        "    return False, stdout.decode('utf-8'), stderr.decode('utf-8')\n",
        "\n",
        "try:\n",
        "    success, stdout, stderr = monitor_vllm_process(vllm_process)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\n‚ùå VLLM server failed to start!\")\n",
        "        print(\"\\nFull STDOUT:\", stdout)\n",
        "        print(\"\\nFull STDERR:\", stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Monitoring interrupted by user\")\n",
        "    stdout, stderr = vllm_process.communicate()\n",
        "    if stdout: print(\"\\nFinal STDOUT:\", stdout.decode('utf-8'))\n",
        "    if stderr: print(\"\\nFinal STDERR:\", stderr.decode('utf-8'))\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co95LrJTl0Yc",
        "outputId": "08b28343-8e10-4576-c93d-36a990af6dfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting VLLM server monitoring...\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:21:00 __init__.py:190] Automatically detected platform cuda.\n",
            "\n",
            "STDERR: 2025-02-09 03:20:55.304365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739071255.551250    1841 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739071255.621097    1841 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-09 03:20:56.124676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:21:02 api_server.py:840] vLLM API server version 0.7.2\n",
            "INFO 02-09 03:21:02 api_server.py:841] args: Namespace(subparser='serve', model_tag='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='half', kv_cache_dtype='auto', max_model_len=16384, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7b515d86f740>)\n",
            "INFO 02-09 03:21:02 api_server.py:206] Started engine process with PID 1946\n",
            "WARNING 02-09 03:21:05 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "\n",
            "STDERR: 2025-02-09 03:21:10.586777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:21:15 __init__.py:190] Automatically detected platform cuda.\n",
            "\n",
            "STDERR: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739071270.627553    1946 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739071270.637791    1946 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: WARNING 02-09 03:21:20 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:21:26 config.py:542] This model supports multiple tasks: {'generate', 'reward', 'score', 'classify', 'embed'}. Defaulting to 'generate'.\n",
            "INFO 02-09 03:21:34 config.py:542] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
            "INFO 02-09 03:21:34 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
            "INFO 02-09 03:21:36 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 02-09 03:21:36 cuda.py:227] Using XFormers backend.\n",
            "INFO 02-09 03:21:37 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
            "INFO 02-09 03:21:38 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
            "INFO 02-09 03:23:03 weight_utils.py:297] No model.safetensors.index.json found in remote.\n",
            "INFO 02-09 03:23:07 model_runner.py:1115] Loading model weights took 3.3460 GB\n",
            "\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.44s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.44s/it]\n",
            "\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:23:11 worker.py:267] Memory profiling takes 2.91 seconds\n",
            "INFO 02-09 03:23:11 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 02-09 03:23:11 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.43GiB; the rest of the memory reserved for KV Cache is 8.44GiB.\n",
            "INFO 02-09 03:23:11 executor_base.py:110] # CUDA blocks: 19758, # CPU blocks: 9362\n",
            "INFO 02-09 03:23:11 executor_base.py:115] Maximum concurrency for 16384 tokens per request: 19.29x\n",
            "\n",
            "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:23:18 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "\n",
            "Capturing CUDA graph shapes:  14%|‚ñà‚ñç        | 5/35 [00:04<00:29,  1.01it/s]\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:23:50 model_runner.py:1562] Graph capturing finished in 33 secs, took 0.19 GiB\n",
            "\n",
            "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:32<00:00,  1.07it/s]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 02-09 03:23:50 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 42.90 seconds\n",
            "INFO 02-09 03:23:51 api_server.py:756] Using supplied chat template:\n",
            "INFO 02-09 03:23:51 api_server.py:756] None\n",
            "INFO 02-09 03:23:51 launcher.py:21] Available routes are:\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /openapi.json, Methods: HEAD, GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /docs, Methods: HEAD, GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /redoc, Methods: HEAD, GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /health, Methods: GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /ping, Methods: POST, GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /tokenize, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /detokenize, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v1/models, Methods: GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /version, Methods: GET\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v1/completions, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v1/embeddings, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /pooling, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /score, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v1/score, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /rerank, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v1/rerank, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /v2/rerank, Methods: POST\n",
            "INFO 02-09 03:23:51 launcher.py:29] Route: /invocations, Methods: POST\n",
            "\n",
            "STDERR: INFO:     Started server process [1841]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "\n",
            "‚úì VLLM server is up and running!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.responses import StreamingResponse\n",
        "import requests\n",
        "\n",
        "# Request schema for input\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "    # model:model now would be passed from the global model.\n",
        "\n",
        "\n",
        "def ask_model(question: str):\n",
        "    \"\"\"\n",
        "    Sends a request to the model server and fetches a response.\n",
        "    \"\"\"\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"  # Adjust the URL if different\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Raise exception for HTTP errors\n",
        "    return response.json()\n",
        "\n",
        "# Usage:\n",
        "result = ask_model(\"Write the code of binary search in python\")\n",
        "print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jlVp0Iim_X7",
        "outputId": "c9175427-3361-42c9-a020-fb84100be658"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-9763e4f2904c4c4288c21ff48fe2e350\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1739071974,\n",
            "  \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"reasoning_content\": null,\n",
            "        \"content\": \"<think>\\nAlright, I need to write the code for a binary search in Python. Let me think about how to approach this.\\n\\nFirst, I remember that binary search works on a sorted list. So, the list should be in order, either ascending or descending. I'll assume it's sorted in ascending order since that's the common case.\\n\\nThe algorithm works by repeatedly dividing the search interval in half. So, I need to keep track of low and high indices. I'll initialize low to 0 and high to the length of the list minus one.\\n\\nThen, while low is less than or equal to high, I calculate the middle index. Middle can be calculated as (low + high) // 2 to avoid floating points.\\n\\nOnce I find the middle element, if it's equal to the target, I return the index. If the middle element is less than the target, I adjust low to middle + 1 because the target must be in the right half. If it's greater, I adjust high to middle - 1 because the target is in the left half.\\n\\nI should test this logic with a sample list to make sure it works. For example, searching for 5 in the list [1, 2, 3, 4, 5]. The steps should correctly identify the position without issues. \\n\\nWait, what about when the list has only one element? The algorithm should handle that by checking the low and high correctly. Also, I should consider cases where the element isn't present to return -1 or some indication that wasn't found.\\n\\nAnother thought: Maybe variables were not declared in the function. I need to define low, high, and middle within the function. Also, error handling? Maybe add print statements to check steps for debugging.\\n\\nI should also think about the time complexity, which is O(log n), so it's efficient even for large lists.\\n\\nPutting it all together, I'll structure the function with these steps, test it with examples, and ensure it handles all edge cases.\\n</think>\\n\\nTo implement the binary search algorithm in Python, we'll create a function that searches for a target value within a sorted list. Here's the step-by-step explanation and the Python code:\\n\\n### Approach\\n1. **Problem Analysis**: Binary search is efficient for a sorted list because it halves the search space at each step. It's ideal for large datasets due to its O(log n) time complexity.\\n\\n2. **Algorithm Details**:\\n   - **Initialization**: Start with `low` set to 0 and `high` set to the length of the list minus one.\\n   - **Loop**: Continue the loop as long as `low` is less than or equal to `high`.\\n   - **Midpoint Calculation**: Compute the middle index `mid = (low + high) // 2`.\\n   - **Comparison**:\\n     - If the list element at `mid` is the target, return `mid`.\\n     - If the list element is less than the target, adjust `low` to `mid + 1`.\\n     - If the list element is greater than the target, adjust `high` to `mid - 1`.\\n   - **Termination**: If the loop ends without finding the target, return -1.\\n\\n### Solution Code\\n```python\\ndef binary_search(arr, target):\\n    low = 0\\n    high = len(arr) - 1\\n    while low <= high:\\n        mid = (low + high) // 2\\n        if arr[mid] == target:\\n            return mid\\n        elif arr[mid] < target:\\n            low = mid + 1\\n        else:\\n            high = mid - 1\\n    return -1\\n```\\n\\n### Explanation\\n- **Function Parameters**: The function `binary_search` accepts a sorted list `arr` and a target value `target`.\\n- **Loop Condition**: The loop continues while `low` is less than or equal to `high` to ensure we have a valid range.\\n- **Mid Calculation**: The midpoint `mid` is calculated to divide the current search interval in half.\\n- **Element Comparison**: Adjust the search range based on whether the target is found at `mid` or in the left or right half.\\n- **Return Result**: Returns the index of the target if found, otherwise returns -1.\\n\\nThis implementation efficiently searches for the target in a sorted list with a time complexity of O(log n), making it suitable for large datasets.\",\n",
            "        \"tool_calls\": []\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 11,\n",
            "    \"total_tokens\": 920,\n",
            "    \"completion_tokens\": 909,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "content = result['choices'][0]['message']['content']\n",
        "pprint.pprint(content, indent=2, width=80, depth=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIucudhxw2is",
        "outputId": "c7523f78-e05d-48b8-cbd5-cdc6cd871bde"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<think>\\n'\n",
            " 'Alright, I need to write the code for a binary search in Python. Let me '\n",
            " 'think about how to approach this.\\n'\n",
            " '\\n'\n",
            " 'First, I remember that binary search works on a sorted list. So, the list '\n",
            " \"should be in order, either ascending or descending. I'll assume it's sorted \"\n",
            " \"in ascending order since that's the common case.\\n\"\n",
            " '\\n'\n",
            " 'The algorithm works by repeatedly dividing the search interval in half. So, '\n",
            " \"I need to keep track of low and high indices. I'll initialize low to 0 and \"\n",
            " 'high to the length of the list minus one.\\n'\n",
            " '\\n'\n",
            " 'Then, while low is less than or equal to high, I calculate the middle index. '\n",
            " 'Middle can be calculated as (low + high) // 2 to avoid floating points.\\n'\n",
            " '\\n'\n",
            " \"Once I find the middle element, if it's equal to the target, I return the \"\n",
            " 'index. If the middle element is less than the target, I adjust low to middle '\n",
            " \"+ 1 because the target must be in the right half. If it's greater, I adjust \"\n",
            " 'high to middle - 1 because the target is in the left half.\\n'\n",
            " '\\n'\n",
            " 'I should test this logic with a sample list to make sure it works. For '\n",
            " 'example, searching for 5 in the list [1, 2, 3, 4, 5]. The steps should '\n",
            " 'correctly identify the position without issues. \\n'\n",
            " '\\n'\n",
            " 'Wait, what about when the list has only one element? The algorithm should '\n",
            " 'handle that by checking the low and high correctly. Also, I should consider '\n",
            " \"cases where the element isn't present to return -1 or some indication that \"\n",
            " \"wasn't found.\\n\"\n",
            " '\\n'\n",
            " 'Another thought: Maybe variables were not declared in the function. I need '\n",
            " 'to define low, high, and middle within the function. Also, error handling? '\n",
            " 'Maybe add print statements to check steps for debugging.\\n'\n",
            " '\\n'\n",
            " \"I should also think about the time complexity, which is O(log n), so it's \"\n",
            " 'efficient even for large lists.\\n'\n",
            " '\\n'\n",
            " \"Putting it all together, I'll structure the function with these steps, test \"\n",
            " 'it with examples, and ensure it handles all edge cases.\\n'\n",
            " '</think>\\n'\n",
            " '\\n'\n",
            " \"To implement the binary search algorithm in Python, we'll create a function \"\n",
            " \"that searches for a target value within a sorted list. Here's the \"\n",
            " 'step-by-step explanation and the Python code:\\n'\n",
            " '\\n'\n",
            " '### Approach\\n'\n",
            " '1. **Problem Analysis**: Binary search is efficient for a sorted list '\n",
            " \"because it halves the search space at each step. It's ideal for large \"\n",
            " 'datasets due to its O(log n) time complexity.\\n'\n",
            " '\\n'\n",
            " '2. **Algorithm Details**:\\n'\n",
            " '   - **Initialization**: Start with `low` set to 0 and `high` set to the '\n",
            " 'length of the list minus one.\\n'\n",
            " '   - **Loop**: Continue the loop as long as `low` is less than or equal to '\n",
            " '`high`.\\n'\n",
            " '   - **Midpoint Calculation**: Compute the middle index `mid = (low + high) '\n",
            " '// 2`.\\n'\n",
            " '   - **Comparison**:\\n'\n",
            " '     - If the list element at `mid` is the target, return `mid`.\\n'\n",
            " '     - If the list element is less than the target, adjust `low` to `mid + '\n",
            " '1`.\\n'\n",
            " '     - If the list element is greater than the target, adjust `high` to `mid '\n",
            " '- 1`.\\n'\n",
            " '   - **Termination**: If the loop ends without finding the target, return '\n",
            " '-1.\\n'\n",
            " '\\n'\n",
            " '### Solution Code\\n'\n",
            " '```python\\n'\n",
            " 'def binary_search(arr, target):\\n'\n",
            " '    low = 0\\n'\n",
            " '    high = len(arr) - 1\\n'\n",
            " '    while low <= high:\\n'\n",
            " '        mid = (low + high) // 2\\n'\n",
            " '        if arr[mid] == target:\\n'\n",
            " '            return mid\\n'\n",
            " '        elif arr[mid] < target:\\n'\n",
            " '            low = mid + 1\\n'\n",
            " '        else:\\n'\n",
            " '            high = mid - 1\\n'\n",
            " '    return -1\\n'\n",
            " '```\\n'\n",
            " '\\n'\n",
            " '### Explanation\\n'\n",
            " '- **Function Parameters**: The function `binary_search` accepts a sorted '\n",
            " 'list `arr` and a target value `target`.\\n'\n",
            " '- **Loop Condition**: The loop continues while `low` is less than or equal '\n",
            " 'to `high` to ensure we have a valid range.\\n'\n",
            " '- **Mid Calculation**: The midpoint `mid` is calculated to divide the '\n",
            " 'current search interval in half.\\n'\n",
            " '- **Element Comparison**: Adjust the search range based on whether the '\n",
            " 'target is found at `mid` or in the left or right half.\\n'\n",
            " '- **Return Result**: Returns the index of the target if found, otherwise '\n",
            " 'returns -1.\\n'\n",
            " '\\n'\n",
            " 'This implementation efficiently searches for the target in a sorted list '\n",
            " 'with a time complexity of O(log n), making it suitable for large datasets.')\n"
          ]
        }
      ]
    }
  ]
}